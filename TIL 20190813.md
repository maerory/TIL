## 20190813 Google Bigquery Ch.4 - Loading Data into BigQuery

### Uploading data into cloud

- Activate your cloud shell by creating a project. The project can automatically link to your Bigquery project

- All the BigQuery commands in terminal starts with 'bq'

- Create Dataset by `bq --location=US mk ch04` (note location is important for billing purposes

- Load the csv to the dataset by

  ```
  bq --location=US load --null_marker=NULL --replace --source_format=CSV --autodetect ch04.college_scorecard ./college_scorecard.csv.gz
  ```

  - Two dashes indicates option, then comes the dataset.table name and the directory of CSV you want to upload

- Prefer streaming over frequent loads if using near real-time data
  
  - Frequent loads lead to significant fragmentation and high metadata overhead
- There is option to use the UI to create table and upload the data
- Dataset is sharable with individua, doman and a Google group



### Unwrangling Data

```sql
select
instnm
, adm_rate_all
, first_gen
, md_faminc
, md_earn_wne_p10
, sat_avg
from
`learn-bigquery-249601.ch04.collge_scorecard` #Apostrophes around the dataset name because it is loaded into project cloud
where
# safe_cast renders null if uncastable
safe_cast(sat_avg as float64) > 1300
and safe_cast(adm_rate_all as float64) < 0.2
and safe_cast(first_gen as float64) > 0.1
order by
cast(md_faminc as float64) asc
```

- Including safe_cast avoids query crashing due to not being able to cast into float64

- All the data are first loaded in as string because there are some string values in the original data ("Privacy") and we have not assigned it in the schema

- Cleaning data before loading - changes "PrivacySuppressed" to NULL in the terminal

  ```
  zless ./college_scorecard.csv.gz|sed 's/PrivacySuppressed/NULL/g'|gzip > /tmp/college_scorecard.csv.
  gz
  ```

- View the schema by:

  ```
  bq show --format prettyjson --schema ch04.college_scorecard (> schema.json # to save the schema)
  ```

- You can also view schema through query in json format:

  ```
  SELECT
  TO_JSON_STRING(
  ARRAY_AGG(STRUCT(
  IF(is_nullable = 'YES', 'NULLABLE', 'REQUIRED') AS mode,
  column_name AS name,
  data_type AS type)
  ORDER BY ordinal_position), TRUE) AS schema
  FROM
  ch04.INFORMATION_SCHEMA.COLUMNS
  WHERE
  table_name = 'college_scorecard'
  ```

- Use the saved and modified schema when loading the data:

  ```
  bq --location=US load --null_marker=NULL --replace --source_format=CSV --schema=schema.json --skip_l
  eading_rows=1 ch04.college_scorecard ./college_scorecard2.csv.gz
  ```

- Now then the previous query becomes much cleaner:

  ```
  SELECT
  INSTNM
  , ADM_RATE_ALL
  , FIRST_GEN
  , MD_FAMINC
  , MD_EARN_WNE_P10
  , SAT_AVG
  FROM
  ch04.college_scorecard
  WHERE
  SAT_AVG > 1300
  AND ADM_RATE_ALL < 0.2
  AND FIRST_GEN > 0.1
  ORDER BY
  MD_FAMINC ASC
  ```

- Create a new table with only necessary columns:

  ```
  CREATE OR REPLACE TABLE ch04.college_scorecard_etl AS
  SELECT
  INSTNM
  , ADM_RATE_ALL
  , FIRST_GEN
  , MD_FAMINC
  , SAT_AVG
  , MD_EARN_WNE_P10
  FROM ch04.college_scorecard
  ```

- Removing the data through cloud terminal

  - `bq rm ch04.college_scorecard` removes the datatable
  - `bq rm -r -f ch04` recursively force remove dataset ch04

- Removing through SQL

  - `DROP TABLE IF EXISTS ch04.college_scorecard_gcs`

  - ```
    ALTER TABLE ch04.college_scorecard
    SET OPTIONS (
    expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 7 DAY),
    description="College Scorecard expires seven days from now"
    )
    ```

- Deleting only specific row

  ```
  DELETE FROM ch04.college_scorecard
  WHERE SAT_AVG IS NUL
  ```

- Inserting new rows

  ```
  INSERT ch04.college_scorecard
  (INSTNM
  , ADM_RATE_ALL
  , FIRST_GEN
  , MD_FAMINC
  , SAT_AVG
  , MD_EARN_WNE_P10
  )
  VALUES ('abc', 0.1, 0.3, 12345, 1234, 23456),
  ('def', 0.2, 0.2, 23451, 1232, 32456)
  ```

- Inserting rows from another table:

  ```
  INSERT ch04.college_scorecard
  SELECT *
  FROM ch04.college_scorecard_etl
  WHERE SAT_AVG IS NULL
  ```

- Copying table (must be done through terminal)

  - `bq cp ch04.college_scorecard someds.college_scorecard_copy`



### Efficiently Loading Data

#### File types

- AVro format : self-describing binary files that are broken into blocks and can be compressed block-by-block
  - can parallelize the loading
  - hierarchical and can represent nested and repeated fields
  - but not human readable
- newline-delimited JSON files
  - hierarchical data, but requires that binary columns be case64-encoded
  - but larger in size than CSV because field name is repeated
- Parquet files
  - binary, block-oriented, compact, and capable of representing hierarchical data
  - stored column-by-column

#### Loading type

- gzippin' - compressed files are faster to transmit and less space, but slower to load into BigQuery, if network is slower, compress.
- Multithread loading
  - `gsutil -m cp *.csv gs://BUCKET/some/location`
  - `bqload ...gs://BUCKET/some/location/*.csv`
- Staging the file on Google Cloud Storage incurs storage costs until loading finishes but relatively cheaper

#### Federated queries and external data sources

- Supported external data sources: Google Cloud Storage, Cloud Bigtable, Cloud SQL and Google Drive

- Using federated queries

  1. Create a table definition using `dq mkdef`

     ```
     bq mkdef --source_format=CSV --autodetect gs://bigquery-oreilly-book/college_sco
     recard.csv
     ```

     - Prints a table definition file, use this table def to make a table

  2. Make a table using `bq mk` passing in the external table def.

     ```
     (append to above)
     > \tmp\mytable.json
     (and create table by)
     bq mk --external_table_definition=/tmp/mytable.json \
     ch04.college_scoreboard
     ```

  3. Query the table

     - Now we access the table via BigQuery directly in Google Cloud Storage

- Wildcards: use to match multiple files that are divided by numbers

  ```
  bq mkdef --source_format=CSV --autodetect gs://bigquery-oreilly-book/college_* > /tmp/mytable.json
  ```

  - Create a table that refers to all the files matched

- Temporary Table

  ```
  LOC="--location US”
  INPUT=gs://bigquery-oreilly-book/college_scorecard.csv
  SCHEMA=$(gsutil cat $INPUT | head -1 | awk -F, '{ORS=”,"}{for (i=1; i <= NF; i++){ print $i”:STRING”; }}' | sed ’s/,$//g'| cut -b 4- )
  
  bq $LOC query --external_table_definition=cstable::${SCHEMA}@CSV=${INPUT} \
  (or specify definion directly by --external_table_definition=cstable::${DEF})
  
  ‘SELECT SUM(IF(SAT_AVG != “NULL”, 1, 0))/COUNT(SAT_AVG) FROM cstable’
  ```

  - Performance might not be ideal

- Parquet and ORC

  - Provide better query performance than CSV or JSON format
  - Don't have to specify a schema since implicit

  ```
  bq mkdef --source_format=PARQUET gs://bucket/dir/files* > table_def.json
  
  bq mk --external_table_definition=table_def.json <dataset>.<table>
  ```

- Hive Partition

  ```
  bq load --source_format=ORC --autodetect
  --hive_partitioning_mode=AUTO <dataset>.<table> <gcs_uri>
  ```

  - GCS URI: encode the table path prefix without including any partition keys in the wildcard

    - gs://some-bucket/some-dir/some-table/* (no need to assign the first common filename)

  - Federated querying requires creation of a table def.

    ```
    bq mkdef --source_format=ORC --autodetect --hive_partitioning_mode=AUTO <gcs_uri> > table_def.json
    ```

  - Other data files are also supported

    ```
    bq mkdef --source_format=NEWLINE_DELIMITED_JSON --autodetect --hive_partitioning_mode=STRINGS <gcs_uri> <schema> > table_def.json
    ```

    - Partitionn keys are auto detected, but not the data types

  - BigQuery's externalk tables are read-only
  - Better over time to move the data to BigQuery's native stroage and rewrite the Hive workload in BigQuery

- When to use federated queries and external data sources

  - Exploratory work using federated queries to determine how best to transform the raw data
  - Keep data in Google Sheets if the spreadsheet will be edited interactively and use federated queries exclusively if the results of those queries need to reflect the live data
  - Keep data in external source if ad-hoc SQL querying of the data is relatveily infrequent
  - For stable, understood datasets that will be updated periodically and queried oftern, native storage is better option

#### Exploratory Work Using Federated Queries

- Best practice is to use self-describing file formats, no need to worry about how BigQuery will interpret the data
- String is the default data type in most cases

```
INPUT=gs://bigquery-oreilly-book/college_scorecard.csv
SCHEMA=$(gsutil cat $INPUT | head -1 | cut -b 4- )
```

- Set up the schema so that we are extracting data type from first line

```
LOC="--location US”
OUTPUT=/tmp/college_scorecard_def.json

bq $LOC mkdef --source_format=CSV --noautodetect $INPUT $SCHEMA \
| sed ’s/"skipLeadingRows”: 0/"skipLeadingRows”: 1/g’ \
| sed ’s/"allowJaggedRows”: false/"allowJaggedRows”: true/g’ \
> $OUTPUT
```

​	Use sed line editor to store it as output

```
# Create a temp function to clean up
CREATE TEMP FUNCTION cleanup_numeric(x STRING) AS (
IF ( x != ‘NULL’ AND x != ‘PrivacySuppressed',
CAST(x as FLOAT64),
NULL)
);

WITH etl_data AS (
SELECT
INSTNM
, cleanup_numeric(ADM_RATE_ALL) AS ADM_RATE_ALL
, cleanup_numeric(FIRST_GEN) AS FIRST_GEN
, cleanup_numeric(MD_FAMINC) AS MD_FAMINC
, cleanup_numeric(SAT_AVG) AS SAT_AVG
, cleanup_numeric(MD_EARN_WNE_P10) AS MD_EARN_WNE_P10
FROM
`ch04.college_scorecard_gcs`
)

SELECT
*
FROM
etl_data
WHERE
SAT_AVG > 1300
AND ADM_RATE_ALL < 0.2
AND FIRST_GEN > 0.1
ORDER BY
MD_FAMINC ASC
LIMIT 10
```

#### ELT in SQL for Experimentation

- If using MySQL with 'somedb' database

  ```
  mysql somedb < select_data.sql | \
  gsutil cp - gs://BUCKET/data_$(date -u “+%F-%T”).tsv
  ```

- where the sql query would be

  ```
  select * from my_table
  where transaction_date >= DATE_SUB(CURDATE(), INTERVAL 10 DAY)
  ```

#### Interactive Exploration using Google Sheets

- Many operations by linking BigQuery and Google Sheets
  - Populating spreadsheet with data from BigQuery
  - Exploering BigQuery tables using sheets
  - Querying Sheets data using SQL
  - 